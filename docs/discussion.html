<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Discussion | Metacognitive asymmetries in visual perception</title>
  <meta name="description" content="4 Discussion | Metacognitive asymmetries in visual perception" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Discussion | Metacognitive asymmetries in visual perception" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Discussion | Metacognitive asymmetries in visual perception" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="results.html"/>
<link rel="next" href="conclusion.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2</b> Methods</a><ul>
<li class="chapter" data-level="2.1" data-path="methods.html"><a href="methods.html#participants"><i class="fa fa-check"></i><b>2.1</b> Participants</a></li>
<li class="chapter" data-level="2.2" data-path="methods.html"><a href="methods.html#procedure"><i class="fa fa-check"></i><b>2.2</b> Procedure</a><ul>
<li class="chapter" data-level="2.2.1" data-path="methods.html"><a href="methods.html#randomization"><i class="fa fa-check"></i><b>2.2.1</b> Randomization</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="methods.html"><a href="methods.html#data-analysis"><i class="fa fa-check"></i><b>2.3</b> Data analysis</a><ul>
<li class="chapter" data-level="2.3.1" data-path="methods.html"><a href="methods.html#analysis-plan"><i class="fa fa-check"></i><b>2.3.1</b> Dependent variables and analysis plan</a></li>
<li class="chapter" data-level="2.3.2" data-path="methods.html"><a href="methods.html#statistical-power"><i class="fa fa-check"></i><b>2.3.2</b> Statistical power</a></li>
<li class="chapter" data-level="2.3.3" data-path="methods.html"><a href="methods.html#rejection-criteria"><i class="fa fa-check"></i><b>2.3.3</b> Rejection criteria</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html#data-availability"><i class="fa fa-check"></i><b>2.4</b> Data availability</a></li>
<li class="chapter" data-level="2.5" data-path="methods.html"><a href="methods.html#code-availability"><i class="fa fa-check"></i><b>2.5</b> Code availability</a></li>
<li class="chapter" data-level="2.6" data-path="methods.html"><a href="methods.html#deviations"><i class="fa fa-check"></i><b>2.6</b> Deviations from pre-registration</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>3</b> Results</a><ul>
<li class="chapter" data-level="3.1" data-path="results.html"><a href="results.html#experiment-1-q-vs.-o"><i class="fa fa-check"></i><b>3.1</b> Experiment 1: <em>Q</em> vs. <em>O</em></a></li>
<li class="chapter" data-level="3.2" data-path="results.html"><a href="results.html#experiment-2-c-vs.-o"><i class="fa fa-check"></i><b>3.2</b> Experiment 2: C vs. O</a></li>
<li class="chapter" data-level="3.3" data-path="results.html"><a href="results.html#experiment-3-tilted-vs.-vertical-lines"><i class="fa fa-check"></i><b>3.3</b> Experiment 3: tilted vs. vertical lines</a></li>
<li class="chapter" data-level="3.4" data-path="results.html"><a href="results.html#experiment-4-curved-vs.-straight-lines"><i class="fa fa-check"></i><b>3.4</b> Experiment 4: curved vs. straight lines</a></li>
<li class="chapter" data-level="3.5" data-path="results.html"><a href="results.html#experiment-5-upward-tilted-vs.-downward-tilted-cubes"><i class="fa fa-check"></i><b>3.5</b> Experiment 5: upward-tilted vs. downward-tilted cubes</a></li>
<li class="chapter" data-level="3.6" data-path="results.html"><a href="results.html#experiment-6-flipped-vs.-normal-letters"><i class="fa fa-check"></i><b>3.6</b> Experiment 6: flipped vs. normal letters</a></li>
<li class="chapter" data-level="3.7" data-path="results.html"><a href="results.html#summary"><i class="fa fa-check"></i><b>3.7</b> Experiments 1-6: summary</a></li>
<li class="chapter" data-level="3.8" data-path="results.html"><a href="results.html#experiment-7-exploratory-grating-vs.-noise"><i class="fa fa-check"></i><b>3.8</b> Experiment 7 (exploratory): grating vs. noise</a></li>
<li class="chapter" data-level="3.9" data-path="results.html"><a href="results.html#exploratory-analysis"><i class="fa fa-check"></i><b>3.9</b> Exploratory analysis</a><ul>
<li class="chapter" data-level="3.9.1" data-path="results.html"><a href="results.html#zroc-analysis"><i class="fa fa-check"></i><b>3.9.1</b> zROC analysis</a></li>
<li class="chapter" data-level="3.9.2" data-path="results.html"><a href="results.html#inter-subject-correlations"><i class="fa fa-check"></i><b>3.9.2</b> Inter-subject correlations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>4</b> Discussion</a></li>
<li class="chapter" data-level="5" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>6</b> References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>A</b> Appendix A</a><ul>
<li class="chapter" data-level="A.1" data-path="appendix-a.html"><a href="appendix-a.html#robustness-region"><i class="fa fa-check"></i><b>A.1</b> Robustness Region</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Metacognitive asymmetries in visual perception</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discussion" class="section level1">
<h1><span class="header-section-number">4</span> Discussion</h1>
<p>In perceptual detection, judgments about the presence or absence of a target stimulus differ in several ways. First, participants are more confident in stimulus presence than in stimulus absence <span class="citation">(e.g., Meuwese et al. 2014; Kellij et al. 2018)</span>. Second, confidence ratings in judgments of stimulus presence are more aligned with objective accuracy <span class="citation">(Meuwese et al. 2014; Kellij et al. 2018; Mazor, Friston, and Fleming 2020)</span>. Finally, participants are faster to report stimulus presence <span class="citation">(Mazor, Friston, and Fleming 2020)</span>. In our positive control detection experiment (Experiment 7) we replicated these detection asymmetries. We found a mean difference of 20% confidence between decisions about the presence or absence of a grating, a metacognitive asymmetry of 0.07 in AUC units (ranging from 0 to 1), and a median difference of 124 milliseconds in response time between reports of target presence and absence.</p>
<p>In six pre-registered experiments, we focused on these three behavioural signatures of decisions about the presence and absence of a stimulus, and asked whether they extend to discrimination tasks where stimuli are distinct in the presence or absence of sub-stimulus features such as the presence of an additional line in a letter, the curvature of a line, or more abstractly, the presence of a surprising default-violating signal. Our six stimulus pairs have been shown in previous studies to produce asymmetries in visual search, potentially reflecting differences in the processing of presences and absences of visual features, and of default-complying versus default-violating stimuli. If detection asymmetries also reflect differences in the abstract processing of presences and absences, or of default-complying versus default-violating sensory input, one would expect to find detection-like asymmetries in response time, confidence, and metacognitive sensitivity for discrimination between stimuli that produce asymmetries in a visual search task.</p>
<p>Starting from the end, Experiments 5 and 6 provide evidence against the proposal that asymmetries in confidence, reaction time and metacognitive sensitivity emerge for default-violating signals at all levels of representation. Stimulus pairs in Exp. 5 (cube orientation) and 6 (letter inversion) produced response-conditional ROC curves that were more consistent with the absence of metacognitive asymmetry than with our prior distribution on effect sizes (see section <a href="methods.html#analysis-plan">2.3.1</a> for the specifics of our Bayesian hypothesis testing, including our prior on effect sizes). Given that these stimuli have been shown to produce reliable asymmetries in visual search <span class="citation">(Von Grünau and Dubé 1994; Shen and Reingold 2001; Malinowski and Hübner 2001; Frith 1974; Wang, Cavanagh, and Green 1994)</span>, we can safely conclude that not all default violations that produce an asymmetry in visual search also produce an asymmetry in metacognitive sensitivity.</p>
<p>Moreover, in Exp. 6, default-complying <em>N</em> responses were faster, and accompanied by higher levels of subjective confidence, than default-violating flipped-<em>N</em> responses. This is in contrast to our prediction of a processing advantage for default-violating signals, and in line with previous reports of a processing advantage for familiar over unfamiliar stimuli in the context of face perception and reading. For example, in a breaking continuous flash suppression (bCFS) paradigm, inverted faces took longer to break into awareness than upright faces <span class="citation">(Stein and Peelen 2021)</span>. A similar processing advantage for familiar stimuli has been documented for the perception of words <span class="citation">(Albonico et al. 2018)</span> and Chinese letters <span class="citation">(Xue et al. 2006)</span>. One possibility is that the perception of highly familiar stimuli such as letters and faces is supported by specific expert brain systems, affording a processing advantage beyond the general superior processing of default-violating signals <span class="citation">(Yovel and Kanwisher 2005; Xue et al. 2006)</span>. Indeed, Exp. 6 was the only experiment in which we observed a processing advantage for familiar over unfamiliar stimuli.</p>
<p>Next, in Experiments 3 and 4 we looked at two features that have a global effect on stimulus appearance: tilt and curvature. Based on visual search asymmetries, <span class="citation">Treisman and Gormican (1988)</span> proposed that tilt and curvature are represented as positive features in the visual system. This takes us one step closer to typical detection experiments: participants now detect the presence or absence of a basic visual feature. In agreement with our Hypotheses 1 and 4, participants were more confident in identifying tilted and curved lines (mean differences of 0.12 and 0.12 on a 0-1 confidence scale), and were faster in giving these responses (mean differences of 67.67 and 50.57 ms). However, we did not find evidence for or against a metacognitive asymmetry for these global visual features.</p>
<p>Our strongest candidate for a stimulus pair for which we expected to find a presence-absence asymmetry was <em>Q</em> vs. <em>O</em> (Exp. 1). The difference between these two letters is the presence of an additional line stroke: a concrete stimulus part that is localized in space and is independent of the rest of the stimulus. Theoretically, participants could approach this task as a detection task: ignore the common denominator (<em>O</em>) and focus on the presence or absence of the distinctive feature (‘,’). As we hypothesized, participants were more confident in their <em>Q</em> responses (mean difference of 0.11 on a 0-1 confidence scale). Participants were also faster in their <em>Q</em> responses (median difference of 37 ms). However, unlike stimulus-level detection, a small difference of 0.04 units in the area under the response conditional ROC curves was not different to what is expected based on a null SDT model.</p>
<p>Finally, In Experiment 2 we looked at discrimination between <em>C</em> and <em>O</em>s based on evidence from visual search that open edges are represented as a positive feature in the visual system <span class="citation">(Treisman and Souther 1985; Takeda and Yagi 2000; Treisman and Gormican 1988)</span>. As we hypothesized, <em>C</em> responses were accompanied by higher levels of subjective confidence (mean difference of 0.05 on a 0-1 confidence scale), and were delivered faster than <em>O</em> responses (with a modest but significant difference of 6 ms between the two responses). However, in striking contrast to our original hypothesis, metacognitive sensitivity was <em>lower</em> for <em>C</em> responses (mean difference of 0.05 AUC units), even when controlling for response bias. This result strongly supports different underlying mechanisms behind search and metacognitive asymmetries. Furthermore, the results of Experiment 2 suggest distinct factors mediate the processing advantage for presence over absence (as reflected in shorter response times and higher confidence for <em>C</em> responses), and the metacognitive asymmetry between presence and absence (as reflected in improved metacognitive sensitivity for <em>O</em> responses).</p>
<p><em>C</em> and <em>O</em> are unique in that the difference between them corresponds to two contrasting notions of presence and absence. On the one hand, <em>C</em> is marked by the presence of one additional feature - open edges <span class="citation">(Treisman and Souther 1985; Treisman and Gormican 1988)</span>. On the other hand, it is marked by the absence of a piece: there is simply less of it relative to <em>O</em>. These two notions of presence and absence are typically coupled in detection. For example, the presence of a grating on a screen corresponds to the presence of additional features (such as orientation, contrast, and phase) as well as of more ‘visual stuff’, relative to the blank background. A compelling interpretation of the results of Exp. 2 is that it is the presence or absence of visual features such as open edges that is driving the difference in confidence and response time, whereas a more quantitative notion of presence or absence (the amount of ‘visual stuff’ presented) is driving the metacognitive asymmetry between these two responses. We note however that based on this interpretation, we would expect a metacognitive sensitivity to operate also in Experiment 1, where <em>O</em> is missing a piece relative to <em>Q</em>. As described above, Experiment 1 provided no evidence for such a metacognitive asymmetry beyond what is expected from an equal-variance signal-detection model.</p>
<p>Notably, not one of the six pre-registered experiments produced a metacognitive asymmetry in the expected direction. This was in contrast to Experiment 7 (grating vs. noise), where metacognitive sensitivity for reporting noise was lower than for reporting a noisy grating (with a difference of 0.07 auROC units, <span class="math inline">\(\mathrm{BF}_{\textrm{10}} = 31.40\)</span>). Positive control Experiment 7 was also the only experiment in which we found higher variance for stimulus S1 than for stimulus S2 (with a median variance ratio of 0.86). These two observations are likely to be related: across participants, metacognitive asymmetry and variance ratio were highly correlated (<span class="math inline">\(r = .64\)</span>, 95% CI <span class="math inline">\([.51\)</span>, <span class="math inline">\(.74]\)</span>, <span class="math inline">\(t(102) = 8.42\)</span>, <span class="math inline">\(p &lt; .001\)</span>). Indeed, previous theoretical work has pointed out that response-dependent asymmetries in metacognition may be driven by an underlying unequal-variance SDT model, and, vice-versa, that findings of unequal variance might be due to a response-dependent metacognitive asymmetry. These two perspectives are interchangeable <span class="citation">(Maniscalco and Lau 2014)</span>. However, a correlation between metacognitive asymmetry and variance structure, both estimated from confidence ratings, is not a satisfactory answer for <em>why</em> noise and gratings should exhibit a unique asymmetry in metacognitive sensitivity, or in variance structure. More theoretical and experimental work is needed to identify the sources of this asymmetry, perhaps focusing on the role of stimulus complexity and perceptual uncertainty as potential drivers of this effect.</p>
<p>When interpreting our findings in a broader context, it is useful to note that in all six experiments we used backward masking for controlling the visibility level of our stimuli. Different visibility manipulations have been shown to affect detection metacognitive sensitivity in different ways. For example, whereas metacognitive sensitivity in detection ‘no’ responses is at chance when backward masking is used, it is significantly higher than chance when the attentional blink is used to control stimulus visibility <span class="citation">(Kanai, Walsh, and Tseng 2010)</span>. Similarly, phase scrambling but not attentional blink produces a metacognitive advantage for ‘yes’ responses <span class="citation">(Kellij et al. 2018)</span>. While our positive control (Exp. 7) produced a reliable metacognitive asymmetry between judgments of target presence and absence, it was also the only experiment where stimulus visibility was controlled with low contrast, in addition to backward masking (for the purpose of compatibility with previous experiments; see Fig. <a href="results.html#fig:rcROC7">3.4</a>). Based on our findings alone, we cannot rule out the possibility that using other visibility manipulations may reveal metacognitive asymmetries for the presence or absence of abstract default violations. Furthermore, it is possible that some of the observed asymmetries for low-level features may reflect asymmetries in the joint perception of target stimulus and backward mask, rather than in the perception of the target stimulus by itself <span class="citation">(Kahneman 1968; Jannati and Di Lollo 2012)</span>.</p>
<p>Together, our findings weigh against our original proposal that metacognitive asymmetries in perceptual detection are a signature of higher-order default reasoning. Unlike search asymmetries that extend to abstract levels of representations such as familiarity <span class="citation">(Wang, Cavanagh, and Green 1994; Wolfe 2001)</span> and even social features such as ethnicity and gender <span class="citation">(Levin and Angelone 2001; Gandolfo and Downing 2020)</span>, metacognitive asymmetries in visual discrimination are grounded in concrete visual processing. Furthermore, we provide evidence for a dissociation between asymmetries in metacognition and in response time and confidence, where the latter is linked to activation of basic feature-detectors, for example of orientation, open ends, or curvature.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="results.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
